{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN TRAIN PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries from Keras, Model and other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy\n",
    "from music21 import converter, instrument, note, chord\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from model import LSTM_model\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main idea - converting all midi files to the large sequence of the notes and chords\n",
    "We have 92 files in midi_songs dir\n",
    "For parsing used music21 library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_notes():\n",
    "    notes = []\n",
    "\n",
    "    midi_files = glob.glob(\"midi_songs/*mid\")\n",
    "    \n",
    "    for idx, item in enumerate(midi_files):\n",
    "        midi_file = converter.parse(item)\n",
    "        if(idx % 10 == 0):\n",
    "            print('{} midi files parsed'.format(idx))\n",
    "        try:\n",
    "            intstument_ = instrument.partitionByInstrument(midi_file)\n",
    "            parsed = instrument_.parts[0].recurse() # parts mean about instruments, our data fully done by the piano\n",
    "        except:\n",
    "            parsed = midi_file.flat.notes\n",
    "        \n",
    "        if(parsed!=None):\n",
    "            for item in parsed:\n",
    "                if isinstance(item, note.Note):\n",
    "                    notes.append(str(item.pitch))\n",
    "                elif isinstance(item, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in item.normalOrder))\n",
    "    with open('data/notes', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 midi files parsed\n",
      "10 midi files parsed\n",
      "20 midi files parsed\n",
      "30 midi files parsed\n",
      "40 midi files parsed\n",
      "50 midi files parsed\n",
      "60 midi files parsed\n",
      "70 midi files parsed\n",
      "80 midi files parsed\n",
      "90 midi files parsed\n"
     ]
    }
   ],
   "source": [
    "parsed = parsing_notes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All unique chords and notes, which we use for making a labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(parsed))  # size of our vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence of features for LSTM will be like that\n",
    "\n",
    "[number of all notes, sequence of 100 notes] \n",
    "\n",
    "output: \n",
    "\n",
    "[number of all notes, note after those sequecne of 100 notes] \n",
    "\n",
    "So, our aim is predict next note or chord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab):\n",
    "    sequence_length = 100 # Fixed Sequence for LSTM, which will be size of input\n",
    "\n",
    "    # get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "    \n",
    "    # create a mapping \"pitch\" : \"int\"\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    input_features = []\n",
    "    output_features = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        input_features.append([note_to_int[char] for char in sequence_in])\n",
    "        output_features.append(note_to_int[sequence_out])\n",
    "        \n",
    "    n_patterns = len(input_features)\n",
    "\n",
    "#     reshape the input into a format compatible with LSTM layers\n",
    "    input_features = numpy.reshape(input_features, (n_patterns, sequence_length, 1))\n",
    "#     normalize input\n",
    "    input_features = input_features / float(n_vocab)\n",
    "\n",
    "    output_features = np_utils.to_categorical(output_features)\n",
    "    return (input_features, output_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = parsed\n",
    "size_vocab = len(set(notes))\n",
    "in_f, out_f = prepare_sequences(notes, size_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of input : LSTM in (60398, 100, 1)\n",
      "Size of output: LSTM out (60398, 359)\n"
     ]
    }
   ],
   "source": [
    "print('Size of input : LSTM in {}'.format(in_f.shape))\n",
    "print('Size of output: LSTM out {}'.format(out_f.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model description in other Model.py file\n",
    "As we can see, when we add new LSTM layers or Dense layers\n",
    "\n",
    "Total parameters increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM = LSTM_model(in_f, size_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 100, 512)          1052672   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 359)               92263     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 359)               0         \n",
      "=================================================================\n",
      "Total params: 5,474,663\n",
      "Trainable params: 5,474,663\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSTM.summary() # OUR DEFAULT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100, 1024)         4202496   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 1024)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100, 1024)         8392704   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 1024)         0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100, 512)          3147776   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 359)               92263     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 359)               0         \n",
      "=================================================================\n",
      "Total params: 18,065,767\n",
      "Trainable params: 18,065,767\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSTM.summary() # ADVANCED MODEL, MORE FEATURES IN NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath,\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    mode='min'\n",
    ")\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   64/60398 [..............................] - ETA: 6:55:22 - loss: 5.9420"
     ]
    }
   ],
   "source": [
    "LSTM.fit(in_f, out_f, epochs=100, batch_size=32, callbacks=callbacks_list) # training part of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train process take a lot of time \n",
    "We trained in Nvidia Quaddro P6000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
